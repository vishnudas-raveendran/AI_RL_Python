{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tic Tac Toe Using Epsilon Greedy RL .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOIHm+G308/Of/DlPxNrTy/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishnudas-raveendran/AI_RL_Python/blob/master/Tic_Tac_Toe_Using_Epsilon_Greedy_RL_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqYLVbkUuM7I",
        "colab_type": "text"
      },
      "source": [
        "## Brief Points on RL\n",
        "  - Data input is not just a static table of data\n",
        "  - An agent interacts with environment in either via simulation or in real world\n",
        "  - An agent has a lifetime\n",
        "  - Continuous stream of data that considers both future and past\n",
        "  - For a large state of possible behaviours in the order of > 10^6 (which is ImageNet's size. It is highly computationally expensive. To illustrate, one order of magnitude higher than 10^6 takes 10 days, 2 order of magnitude takes 100 days and so on.\n",
        "      - Also, RL allows 'creativity' from the machine in solving a task\n",
        "\n",
        "## Terms\n",
        "  **Rewards**\n",
        "\n",
        "  - The goal of RL is to maximize the rewards and has an eye to the future, even if it makes a low reward decision for short term.\n",
        "  - Unlike, supervised learning - we are not trying to minimize cost on current input\n",
        "  - Rewards (feedback) comes from the environment\n",
        "  - The RL algo would not know if the decision taken is right or wrong until it gets a reward in the end of the game\n",
        "\n",
        "**Agent**\n",
        "\n",
        "Actor which interacts with the environment. We are trying to teach this agent.\n",
        "\n",
        "**Environment**\n",
        "\n",
        "Real or simulated world of the agent\n",
        "\n",
        "**State**\n",
        "\n",
        "Different configurations of the environment which the agent can sense\n",
        "\n",
        "**State action triple**\n",
        "\n",
        "Timing is an important parameter in the RL world. An gent moves from S(t) to S(t+1) by doing an action A\n",
        "\n",
        "This is denoted as \\[S(t),A(t),S(t+1)\\] or (s,a,s')\n",
        "\n",
        "Now, s' does not necessarily mean the state at time t+1 but instead it could simply mean the next state arrived after executing action A at time t.\n",
        "\n",
        "Another version of the same is a **4-tuple** representation where (s,a,r,s') denotes the state s' reached by agent from state s on performing an action a with a reward r.\n",
        "\n",
        "**Episode**\n",
        "\n",
        "Episode represents one run of the agent going through all states. In terms of tic-tac-toe, one episode is one game.\n",
        "\n",
        "In contrast is a **Continous task** which never ends. In episodic task we play again and again.\n",
        "\n",
        "**Terminal State**\n",
        "\n",
        "Is one from where there are no more actions to go to another state. It signals an end of an episode.\n",
        "\n",
        "## Value functions\n",
        "\n",
        "The reward given at the end of the episode tells the agent about its past actions. This is called the **credit assignment problem**.\n",
        "\n",
        "So, an agent would make the next decision with memory of which action gave the reward. *The action is based on past.*\n",
        "\n",
        "Related, is the concept of **delayed rewards**. Here, the agent has an eye to the future. The agent gives a value to the current state based on the rewards he can get being by going to the next state from the current state. The delayed rewards is based on achieving a global optimum.\n",
        "\n",
        "Therefore, delayed rewards gives a methodology and a measure to decide the next course of action from a state. This is related to the field of planning. The measure is done using the **value function**. The value thus gives a measure of the \"future goodness of the state\"\n",
        "\n",
        "The value function is similar to the gradient decent formula\n",
        "\n",
        "V(S) <- V(S) + alpha(V(S) - V(S'))\n",
        "\n",
        "where, alpha is the learning rate, V(S') is the value of the next state, V(S) is the value of the current state.\n",
        "\n",
        "As with gradient descent, the V(S') is the accurate true value of the state and idea is that by iterating over many episodes, the V(S) will eventually approach (converge to) V(S').\n",
        "\n",
        "However, here we are riding under the wrongful assumption that V(S') is accurate. It is not, we have not found V(S') yet. Had it been supervised learning, we would have had V(S') values which we know as correct. Here, we have to find out V(S') too. \n",
        "\n",
        "This is therefore the same dilemma (explore-exploit) as the slot machines (bandit problem), how can we pull the correct lever without knowing which machine gives a good chance of a win. \n",
        "\n",
        "Thus, we use epsilon greedy strategy to decide between choosing a random location to mark or choose the best location from V(S) states in the board.\n",
        "\n",
        "\n",
        "**Initialisation of Value funtion**\n",
        "\n",
        "[to-do after writing the tic-tac-toe program]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRl6TdwUgqft",
        "colab_type": "text"
      },
      "source": [
        "## Questions for clarity\n",
        "\n",
        "*Why order of finding value function is important? Possibly, on the same thread, why are we going backwards through state_history?*\n",
        "\n",
        "Answer: Based on what I concur and understand from similarly other question is that, in this particular game we are defining the final state or 1 (won game), 0 (lost game) , 0.5 (draw game) so we know with certainity. Next, from the terminal state, we go backwards in each step to reassign the values of each state in the board. Overtime, this backward (propagation?) gives a good value for V(S')."
      ]
    }
  ]
}